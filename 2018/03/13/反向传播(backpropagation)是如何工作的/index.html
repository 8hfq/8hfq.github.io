<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="反向传播(backpropagation)是如何工作的反向传播算法最早于上世纪70年代被提出，但是直到1986年，由David Rumelhart, Geoffrey Hinton, 和Ronald Williams联合发表了一篇著名论文之后，人们才完全认识到这个算法的重要性。这篇论文介绍了几种神经网络，在这些网络的学习中，反向传播算法比之前提出的方法都要快。这使得以前用神经网络不可解的一些问题，">
<meta property="og:type" content="article">
<meta property="og:title" content="八号风球的博客">
<meta property="og:url" content="http://yoursite.com/2018/03/13/反向传播(backpropagation)是如何工作的/index.html">
<meta property="og:site_name" content="八号风球的博客">
<meta property="og:description" content="反向传播(backpropagation)是如何工作的反向传播算法最早于上世纪70年代被提出，但是直到1986年，由David Rumelhart, Geoffrey Hinton, 和Ronald Williams联合发表了一篇著名论文之后，人们才完全认识到这个算法的重要性。这篇论文介绍了几种神经网络，在这些网络的学习中，反向传播算法比之前提出的方法都要快。这使得以前用神经网络不可解的一些问题，">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2018/03/13/反向传播(backpropagation)是如何工作的/media/15205765858545/15205777820744.png">
<meta property="og:image" content="http://yoursite.com/2018/03/13/反向传播(backpropagation)是如何工作的/media/15205765858545/15205799489526.png">
<meta property="og:image" content="http://yoursite.com/2018/03/13/反向传播(backpropagation)是如何工作的/media/15205765858545/15205780111447.png">
<meta property="og:image" content="http://yoursite.com/2018/03/13/反向传播(backpropagation)是如何工作的/media/15205765858545/15209122645147.jpg">
<meta property="og:image" content="http://yoursite.com/2018/03/13/反向传播(backpropagation)是如何工作的/media/15205765858545/15209122861731.jpg">
<meta property="og:image" content="http://yoursite.com/2018/03/13/反向传播(backpropagation)是如何工作的/media/15205765858545/15209123666653.jpg">
<meta property="og:image" content="http://yoursite.com/2018/03/13/反向传播(backpropagation)是如何工作的/media/15205765858545/15209123798567.jpg">
<meta property="og:image" content="http://yoursite.com/2018/03/13/反向传播(backpropagation)是如何工作的/media/15205765858545/15205806226972.png">
<meta property="og:image" content="http://yoursite.com/2018/03/13/反向传播(backpropagation)是如何工作的/media/15205765858545/15205816370977.png">
<meta property="og:image" content="http://yoursite.com/2018/03/13/反向传播(backpropagation)是如何工作的/media/15205765858545/15205816703098.png">
<meta property="og:image" content="http://yoursite.com/2018/03/13/反向传播(backpropagation)是如何工作的/media/15205765858545/15205816827768.png">
<meta property="og:image" content="http://yoursite.com/2018/03/13/反向传播(backpropagation)是如何工作的/media/15205765858545/15205816899701.png">
<meta property="og:image" content="http://yoursite.com/2018/03/13/反向传播(backpropagation)是如何工作的/media/15205765858545/15205817090916.png">
<meta property="og:image" content="http://yoursite.com/2018/03/13/反向传播(backpropagation)是如何工作的/media/15205765858545/15205817218691.png">
<meta property="og:updated_time" content="2018-03-13T03:40:03.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="八号风球的博客">
<meta name="twitter:description" content="反向传播(backpropagation)是如何工作的反向传播算法最早于上世纪70年代被提出，但是直到1986年，由David Rumelhart, Geoffrey Hinton, 和Ronald Williams联合发表了一篇著名论文之后，人们才完全认识到这个算法的重要性。这篇论文介绍了几种神经网络，在这些网络的学习中，反向传播算法比之前提出的方法都要快。这使得以前用神经网络不可解的一些问题，">
<meta name="twitter:image" content="http://yoursite.com/2018/03/13/反向传播(backpropagation)是如何工作的/media/15205765858545/15205777820744.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/03/13/反向传播(backpropagation)是如何工作的/"/>





  <title> | 八号风球的博客</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">八号风球的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/13/反向传播(backpropagation)是如何工作的/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="E.Wind">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="八号风球的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline"></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-13T11:40:03+08:00">
                2018-03-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="反向传播-backpropagation-是如何工作的"><a href="#反向传播-backpropagation-是如何工作的" class="headerlink" title="反向传播(backpropagation)是如何工作的"></a>反向传播(backpropagation)是如何工作的</h1><p>反向传播算法最早于上世纪70年代被提出，但是直到1986年，由David Rumelhart, Geoffrey Hinton, 和Ronald Williams联合发表了一篇著名论文之后，人们才完全认识到这个算法的重要性。这篇论文介绍了几种神经网络，在这些网络的学习中，反向传播算法比之前提出的方法都要快。这使得以前用神经网络不可解的一些问题，现在可以通过神经网络来解决。今天，反向传播算法是神经网络学习过程中的关键（workhorse）所在。<br>反向传播算法的核心是一个偏微分表达式，表示代价函数对网络中的权重（或者偏置）求偏导。这个式子告诉我们，当我们改变权重和偏置的时候，代价函数的值变化地有多快。尽管这个式子有点复杂，这个式子也是很漂亮的，它的每一个部分都有自然的，直觉上的解释。因此，反向传播不仅仅是一种快速的学习算法，它能够让我们详细深入地了解改变权重和偏置的值是如何改变整个网络的行为的。这是非常值得深入学习的。</p>
<h2 id="关于代价函数的两个架设"><a href="#关于代价函数的两个架设" class="headerlink" title="关于代价函数的两个架设"></a>关于代价函数的两个架设</h2><p>反向传播算法的目标是计算代价函数对神经网络中出现的所有权重和偏置的偏导数和</p>
<h5 id="第一条假设是代价函数能够被写成的形式，其中是每个独立训练样本的代价函数。在代价函数为平方代价函数的情况下，一个训练样本的代价是。该假设对于本书中涉及到的其它所有代价函数都成立。"><a href="#第一条假设是代价函数能够被写成的形式，其中是每个独立训练样本的代价函数。在代价函数为平方代价函数的情况下，一个训练样本的代价是。该假设对于本书中涉及到的其它所有代价函数都成立。" class="headerlink" title="第一条假设是代价函数能够被写成的形式，其中是每个独立训练样本的代价函数。在代价函数为平方代价函数的情况下，一个训练样本的代价是。该假设对于本书中涉及到的其它所有代价函数都成立。"></a>第一条假设是代价函数能够被写成的形式，其中是每个独立训练样本的代价函数。在代价函数为平方代价函数的情况下，一个训练样本的代价是。该假设对于本书中涉及到的其它所有代价函数都成立。</h5><p>我们需要上述假设的原因是，反向传播实际上是对单个训练数据计算偏导数和。然后通过对所有训练样本求平均值获得和。事实上，有了这个假设，我们可以认为训练样本是固定的，然后把代价去掉下标表示为。最终我们会重新把加回公式，但目前为了简便我们将它隐去。</p>
<h5 id="第二条假设是它可以写成关于神经网络输出结果的函数："><a href="#第二条假设是它可以写成关于神经网络输出结果的函数：" class="headerlink" title="第二条假设是它可以写成关于神经网络输出结果的函数："></a>第二条假设是它可以写成关于神经网络输出结果的函数：</h5><p><img src="media/15205765858545/15205777820744.png" alt=""><br>平方代价函数满足该要求，因为单一训练样本的二次代价可以表示为：<br>这是一个关于输出激活值的函数。显然，该代价函数也依赖于期望的输出，所以你可能疑惑为什么我们不把代价视为关于的函数。记住，输入的训练样本是固定的，因此期望的输出也是固定的。需要注意，我们不能通过改变权值或偏置来修改它，换句话说，它不是神经网络所学习的东西。所以把视为只关于输出的函数是有道理的。在该函数中只是帮助定义函数的参数。</p>
<h2 id="Hadamard积"><a href="#Hadamard积" class="headerlink" title="Hadamard积"></a>Hadamard积</h2><p>反向传播算法是以常见线性代数操作为基础——诸如向量加法，向量与矩阵乘法等运算。但其中一个操作相对不是那么常用。具体来讲，假设和是两个有相同维数的向量。那么我们用来表示两个向量的对应元素(elementwise)相乘。因此的元素。例如，<br>这种对应元素相乘有时被称为Hadamard积（Hadamard product）或Schur积(Schur product)。我们将称它为Hadamard积。优秀的矩阵库通常会提供Hadamard积的快速实现，这在实现反向传播时将会有用。</p>
<h2 id="反向传播背后的四个基本等式"><a href="#反向传播背后的四个基本等式" class="headerlink" title="反向传播背后的四个基本等式"></a>反向传播背后的四个基本等式</h2><p>反向传播(backpropagation)能够帮助解释网络的权重和偏置的改变是如何改变代价函数的。归根结底，它的意思是指计算偏导数和。但是为了计算这些偏导数，我们首先介绍一个中间量，，我们管它叫做层的神经元的错误量(error)。反向传播会提供给我们一个用于计算错误量的流程，能够把和、关联起来。<br>为了理解错误量是如何定义的，想象一下在我们的神经网络中有一个恶魔：<br><img src="media/15205765858545/15205799489526.png" alt=""></p>
<p>这个恶魔位于层的神经元。当神经元的输入进入时，这个恶魔扰乱神经元的操作。它给神经元的加权输入添加了一点改变，这就导致了神经元的输出变成了，而不是之前的。这个改变在后续的网络层中传播，最终使全部代价改变了。<br>而今，这个恶魔变成了一个善良的恶魔，它试图帮助你改善代价，比如，它试图找到一个能够让代价变小。假设是一个很大的值（或者为正或者为负）。然后这个善良的恶魔可以通过选择一个和符号相反的使得代价降低。相比之下，如果接近于0，那么这个恶魔几乎不能通过扰乱加权输入改善多少代价。在一定范围内这个善良的恶魔就可以分辨出，这个神经元已经接近于最佳状态1。至此，有了一种启发式的感觉：可以用来衡量神经元里的错误量。</p>
<p><img src="media/15205765858545/15205780111447.png" alt=""></p>
<h5 id="1-输出层中关于错误量的等式（计算最后一层神经网络产生的错误）："><a href="#1-输出层中关于错误量的等式（计算最后一层神经网络产生的错误）：" class="headerlink" title="1.输出层中关于错误量的等式（计算最后一层神经网络产生的错误）："></a>1.输出层中关于错误量的等式（计算最后一层神经网络产生的错误）：</h5><p>推导过程 <img src="media/15205765858545/15209122645147.jpg" alt=""></p>
<p>这是一种非常自然的表达。右侧的第一项，就是用于测量输出激活代价改变有多快的函数。举个例子，如果并不太依赖于某个特别的输出神经元，那么就会很小，这是我们所期望的。右侧的第二项，用于测量处的激活函数改变有多快。<br>你应该注意到(BP1)中的每一项都是容易计算的。特别的，当计算神经网络的行为时就计算了，而计算也仅仅是一小部分额外的开销。当然了，的确切形式依赖于代价函数的形式。然而，如果提供了代价函数，大家也应该知道计算也不会有什么困难。举个例子，如果我们使用平方代价函数，即，那么，这显然很容易计算。<br>等式(BP1)是的分量形式。它是一个完美的表达式，但并不是我们想要的基于矩阵的形式，那种矩阵形式可以很好的用于反向传播。然而，我们可以很容易把等式重写成基于矩阵的形式，就像：<br>其中，是一个向量，它是由组成的。你可以把看做现对于输出激活的的改变速率。很容易看出来等式(BP1)和(BP1a)是等价的，基于这个原因我们从现在开始将使用(BP1)交替地指代两个等式。举个例子，在使用平方代价函数的情况下我们有，所以完整的基于矩阵的(BP1)的形式变为：<br>就像你所看到的，表达式里的每一项都拥有一个漂亮的向量形式，并且很容易使用一个库来计算，比如Numpy。</p>
<h5 id="2-依据下一层错误量获取错误量的等式（由后往前，计算每一层神经网络产生的错误）："><a href="#2-依据下一层错误量获取错误量的等式（由后往前，计算每一层神经网络产生的错误）：" class="headerlink" title="2.依据下一层错误量获取错误量的等式（由后往前，计算每一层神经网络产生的错误）："></a>2.依据下一层错误量获取错误量的等式（由后往前，计算每一层神经网络产生的错误）：</h5><p><img src="media/15205765858545/15209122861731.jpg" alt=""></p>
<p>其中，是层的权重矩阵的转置。这个等式看着有些复杂，但是每一项都有很好的解释。假设我们知道层的错误量。当我们使用转置权值矩阵的时候，我们可以凭借直觉认为将错误反向（backward）移动穿过网络，带给我们某种测量层输出的错误量方法。然后我们使用Hadamard乘积。这就是将错误量反向移动穿过层的激活函数，产生了层的加权输入的错误量。<br>通过结合(BP2)和(BP1)我们可以计算网络中任意一层的错误量。我们开始使用(BP1)来计算，然后应用等式(BP2)来计算，然后再次应用等式(BP2)来计算，以此类推，反向通过网络中的所有路径。</p>
<h5 id="3-网络的代价函数相对于偏置的改变速率的等式（计算偏置的梯度）："><a href="#3-网络的代价函数相对于偏置的改变速率的等式（计算偏置的梯度）：" class="headerlink" title="3.网络的代价函数相对于偏置的改变速率的等式（计算偏置的梯度）："></a>3.网络的代价函数相对于偏置的改变速率的等式（计算偏置的梯度）：</h5><p><img src="media/15205765858545/15209123666653.jpg" alt=""></p>
<p>也就是说，错误量完全等于改变速率。这是一个很好的消息，因为(BP1)和(BP2)已经告诉我们如何计算。我们把(BP3)重写成如下的简略形式：<br>这可以理解成可以和偏置在相同的神经元中被估计。</p>
<h5 id="4-网络的代价函数相对于权重的改变速率的等式（计算权重的梯度）：："><a href="#4-网络的代价函数相对于权重的改变速率的等式（计算权重的梯度）：：" class="headerlink" title="4.网络的代价函数相对于权重的改变速率的等式（计算权重的梯度）：："></a>4.网络的代价函数相对于权重的改变速率的等式（计算权重的梯度）：：</h5><p><img src="media/15205765858545/15209123798567.jpg" alt=""></p>
<p>这个等式告诉我们如何依据和来计算偏导，而这两个量我们已经知道如何计算了。这个等式可以重写成如下含有少量下标的形式：<br>可以这么理解，是神经元的激活量，输入到权重中，是神经元的错误量，从权重输出。观察这个权重，两个神经元通过这个权重连接起来，我们可以这样描画出来：<br><img src="media/15205765858545/15205806226972.png" alt=""></p>
<p>等式(32)的一个很好的结论是当激活量很小的时候，，梯度项也将会趋近于很小。在这种情况下，我们说权重学习得很慢，也就是说在梯度下降的时候并没有改变很多。换而言之，等式(BP4)的一个结果就是从低激活量神经元里输出的权重会学习缓慢。</p>
<h2 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h2><p>反向传播等式为我们提供了一个计算代价函数梯度的方法。下面让我们明确地写出该算法：</p>
<ul>
<li>输入 :计算输入层相应的激活函数值。</li>
<li>正向传播：对每个，计算和。</li>
<li>输出误差 ：计算向量。</li>
<li>将误差反向传播：对每个计算</li>
<li>输出：代价函数的梯度为和<br>通过以上算法就能看出它为什么叫反向传播算法。我们从最后一层开始，反向计算错误向量。在神经网络中反向计算误差可能看起来比较奇怪。但如果回忆反向传播的证明过程，会发现反向传播的过程起因于代价函数是关于神经网络输出值的函数。为了了解代价函数是如何随着前面的权重和偏置改变的，我们必须不断重复应用链式法则，通过反向的计算得到有用的表达式。</li>
</ul>
<h2 id="为什么说反向传播算法很高效？"><a href="#为什么说反向传播算法很高效？" class="headerlink" title="为什么说反向传播算法很高效？"></a>为什么说反向传播算法很高效？</h2><p>为什么说反向传播算法很高效？要回答这个问题，让我们来考虑另一种计算梯度的方式。设想现在是神经网络研究的早期阶段，大概是在上世纪50年代或60年代左右，并且你是第一个想到使用梯度下降方法来进行训练的人！但是要实现这个想法，你需要一种计算代价函数梯度的方式。你回想了你目前关于演算的知识，决定试一下是否能用链式法则来计算梯度。但是琢磨了一会之后发现，代数计算看起来非常复杂，你也因此有些失落。所以你尝试着寻找另一种方法。你决定把代价单独当做权重的函数（我们一会再来讨论偏置）。将权重写作，并且要对某个权重计算。一个很明显的计算方式是使用近似：<br>其中是一个大于零的小正数。换句话说，我们可以通过计算两个差距很小的wj的代价，然后利用等式(46)来估计。我们可以利用相同的思想来对偏置求偏导。<br>这种方式看起来很不错。它的概念很简单，实现起来也很简单，只需要几行代码就可以。当然了，他看起来要比使用链式法则来计算梯度靠谱多了！<br>然而遗憾的是，虽然这种方式看起来很美好，但当用代码实现之后就会发现，它实在是太慢了。要理解其中的原因的话，设想在我们的神经网络中有一百万个权重，对于每一个不同的权重，为了计算，我们需要计算。这意味着为了计算梯度，我们需要计算一百万次代价函数，进而对于每一个训练样例，都需要在神经网络中前向传播一百万次。我们同样需要计算，因此总计需要一百万零一次前向传播。<br>反向传播的优点在于它仅利用一次前向传播就可以同时计算出所有的偏导，随后也仅需要一次反向传播。大致来说，反向传播算法所需要的总计算量与两次前向传播的计算量基本相等（这应当是合理的，但若要下定论的话则需要更加细致的分析。合理的原因在于前向传播时主要的计算量在于权重矩阵的乘法计算，而反向传播时主要的计算量在于权重矩阵转置的乘法。很明显，它们的计算量差不多）。这与基于等式(46)的方法所需要的一百万零一次前向传播相比，虽然反向传播看起来更复杂一些，但它确实更更更更更快。<br>这种加速方式在1986年首次被人们所重视，极大地拓展了神经网络能够适用的范围，也导致了神经网络被大量的应用。当然了，反向传播算法也不是万能的。在80年代后期，人们终于触及到了性能瓶颈，在利用反向传播算法来训练深度神经网络（即具有很多隐含层的网络）时尤为明显。在本书后面的章节中我们将会看到现代计算机以及一些非常聪明的新想法是如何让反向传播能够用来训练深度神经网络的。</p>
<h2 id="反向传播：整体描述"><a href="#反向传播：整体描述" class="headerlink" title="反向传播：整体描述"></a>反向传播：整体描述</h2><p>正如我之前所阐述的，反向传播涉及了两个谜题。第一个谜题是，这个算法究竟在做什么？我们之前的描述是将错误量从输出层反向传播。但是，我们是否能够更加深入，对这些矩阵、向量的乘法背后作出更加符合直觉的解释？第二个谜题是，人们一开始是如何发现反向传播算法的？按照算法流程一步步走下来，或者证明算法的正确性，这是一回事。但这并不代表你能够理解问题的本质从而能够从头发现这个算法。是否有一条合理的思维路线使你能够发现反向传播算法？在本节中，我会对这两个谜题作出解释。<br>为了更好地构建的反向传播算法在做什么的直觉，让我们假设我们对网络中的某个权重做出了一个小的改变量：<br><img src="media/15205765858545/15205816370977.png" alt=""></p>
<p>这个改变量会导致与其相关的神经元的输出激活值的改变：<br><img src="media/15205765858545/15205816703098.png" alt=""></p>
<p>以此类推，会引起下一层的所有激活值的改变：<br><img src="media/15205765858545/15205816827768.png" alt=""></p>
<p>这些改变会继续引起再下一层的改变、下下层…依次类推，直到最后一层，然后引起代价函数的改变：<br><img src="media/15205765858545/15205816899701.png" alt=""></p>
<p>代价函数的改变量与最初权重的改变量是有关的，关系是下面这个等式<br>这表明了计算的一种可能方法是，计算上的一个小改变量经过正向传播，对引起了多大的改变量。如果我们能够通过小心翼翼的计算做到这点，那我们就可以计算出。<br>让我们尝试来写一下计算过程。改变量对层的第个神经元的激活值带来了的改变量。它的大小是<br>激活值改变量会引起下一层（第层）的所有激活值都改变。我们先关注其中的一个结点，，<br><img src="media/15205765858545/15205817090916.png" alt=""></p>
<p>它产生的改变量是：<br>将等式(48)带入其中，得到：<br>当然，改变量会继续造成下一层的激活值的改变。实际上，我们可以想象一条从到的路径，其中每一个结点的激活值的改变都会引起下一层的激活值的改变，最终引起输出层的代价的改变。如果这条路径是，那么最终的改变量是<br>这样，我们使用了一系列的形式的项，对应了路径上的每一个结点，包括最终项。这就计算出了在神经网络的这条路径上，最初的改变量引起了多大的改变。当然，由的改变量影响代价改变的路径选择是很多的，我们现在只考虑了其中的一条路径。为了计算最终总共的改变量，很显然我们应该对所有可能的路径对其带来的改变量进行求和：<br>其中，我们对每条路径中所有出现的神经元都进行求和。与等式(47)进行比较，我们得到：<br>等式(53)看上去很复杂。不过，它在直觉上很容易理解。我们计算出了相对于网络中的一个权重的变化速率。这个等式告诉我们的是，每一条连接两个神经元的边都可以对应一个变化速率，这个变化速率的大小是后一个神经元对前一个神经元的偏导。连接第一个权重和第一个神经元的边对应的变化速率是。一条路径对应的变化速率恰好是路径上的变化速率的连乘。总变化速率是从起始权重到最终代价上的所有可能的路径的变化速率的总和。用图片来说明这个过程，对于一条路径来说：<br><img src="media/15205765858545/15205817218691.png" alt=""></p>
<p>目前为止我所阐述的是一种启发式的观点，当你困惑于神经网络中的权重时，可以通过这种观点来思考。下面我会给你一些简要的思路使你可以更进一步的完善这个观点。首先，你可以显式地计算出等式53中的每一项的偏导表达式。这很容易做到，只需要一点计算量就行。做完之后，你可以尝试将所有的求和通过矩阵的形式来表示。这个过程会有一点无聊，并且需要一些毅力，但是不会特别的困难。随后，你可以尝试尽可能的将表达式简化，最终你会发现你得到了反向传播算法！所以，你可以将反向传播算法看作是一种对所有路径上的所有变化率进行求和的方法。或者用另外一种方式来说，反向传播算法是一种很聪明的方法，当小扰动沿着网络传播、到达输出并影响代价的过程中，它能够记录其对相应权重（和偏置）的影响量。<br>我的解释到此为止了。这可能有点复杂，并且需要仔细思考所有的细节。如果你乐于接受挑战，你可能会很享受这个过程。如果不是，我希望我的这些想法能够给你一些关于反向传播在做什么的启发。<br>那关于其它的谜题呢——反向传播最初是如何被发现的？实际上，如果你一路看下来我的阐述，你能够找到关于反向传播算法的一种证明。不幸的是，完整的证明实际上比我在本章的描述更长更复杂。那这个相对更简单（但更玄虚）的证明是如何发现的呢？当你把试图把完整的证明中的所有细节写出来时，你会发现有一些很显然能够被简化的形式。你做完这些简化，会得到一个简短一些的证明。然后你又会发现一些可以简化的内容。当你重复几次这个过程之后，你会得到本章中的这个简短的证明，但是它有点晦涩，因为其中所有复杂的结构都被简化掉了！我希望你能够相信我，完整的证明与本章中简短的证明没有什么本质区别。我只是对证明过程做了很多简化的工作。</p>
<p>转自gitbook  Neural Networks and Deep Learning 翻译<a href="https://www.gitbook.com/book/hit-scir/neural-networks-and-deep-learning-zh_cn/details" target="_blank" rel="noopener">https://www.gitbook.com/book/hit-scir/neural-networks-and-deep-learning-zh_cn/details</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/07/20/详解神经网络/" rel="prev" title="详解神经网络">
                详解神经网络 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">E.Wind</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#反向传播-backpropagation-是如何工作的"><span class="nav-number">1.</span> <span class="nav-text">反向传播(backpropagation)是如何工作的</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#关于代价函数的两个架设"><span class="nav-number">1.1.</span> <span class="nav-text">关于代价函数的两个架设</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#第一条假设是代价函数能够被写成的形式，其中是每个独立训练样本的代价函数。在代价函数为平方代价函数的情况下，一个训练样本的代价是。该假设对于本书中涉及到的其它所有代价函数都成立。"><span class="nav-number">1.1.0.0.1.</span> <span class="nav-text">第一条假设是代价函数能够被写成的形式，其中是每个独立训练样本的代价函数。在代价函数为平方代价函数的情况下，一个训练样本的代价是。该假设对于本书中涉及到的其它所有代价函数都成立。</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#第二条假设是它可以写成关于神经网络输出结果的函数："><span class="nav-number">1.1.0.0.2.</span> <span class="nav-text">第二条假设是它可以写成关于神经网络输出结果的函数：</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hadamard积"><span class="nav-number">1.2.</span> <span class="nav-text">Hadamard积</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#反向传播背后的四个基本等式"><span class="nav-number">1.3.</span> <span class="nav-text">反向传播背后的四个基本等式</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-输出层中关于错误量的等式（计算最后一层神经网络产生的错误）："><span class="nav-number">1.3.0.0.1.</span> <span class="nav-text">1.输出层中关于错误量的等式（计算最后一层神经网络产生的错误）：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-依据下一层错误量获取错误量的等式（由后往前，计算每一层神经网络产生的错误）："><span class="nav-number">1.3.0.0.2.</span> <span class="nav-text">2.依据下一层错误量获取错误量的等式（由后往前，计算每一层神经网络产生的错误）：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-网络的代价函数相对于偏置的改变速率的等式（计算偏置的梯度）："><span class="nav-number">1.3.0.0.3.</span> <span class="nav-text">3.网络的代价函数相对于偏置的改变速率的等式（计算偏置的梯度）：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-网络的代价函数相对于权重的改变速率的等式（计算权重的梯度）：："><span class="nav-number">1.3.0.0.4.</span> <span class="nav-text">4.网络的代价函数相对于权重的改变速率的等式（计算权重的梯度）：：</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#反向传播算法"><span class="nav-number">1.4.</span> <span class="nav-text">反向传播算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#为什么说反向传播算法很高效？"><span class="nav-number">1.5.</span> <span class="nav-text">为什么说反向传播算法很高效？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#反向传播：整体描述"><span class="nav-number">1.6.</span> <span class="nav-text">反向传播：整体描述</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">E.Wind</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
