<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[专业课复习内容]]></title>
    <url>%2F2018%2F07%2F23%2F%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%A4%8D%E4%B9%A0%E5%86%85%E5%AE%B9%2F</url>
    <content type="text"><![CDATA[《软件工程专业基础综合》考试大纲 第一部分 数据结构与算法考试题型：问答、分析、编程 总分：60分 一、栈（Stack）、队列（Queue）和向量（Vector）内容: 单链表,双向链表,环形链表,带哨兵节点的链表; 栈的基本概念和性质,栈ADT及其顺序,链接实现;栈的应用;栈与递归; 队列的基本概念和性质,队列ADT及其顺序,链接实现;队列的应用; 向量基本概念和性质;向量ADT及其数组、链接实现; 二、树内容: 树的基本概念和术语;树的前序,中序,后序,层次序遍历; 二叉树及其性质;普通树与二叉树的转换; 树的存储结构,标准形式;完全树(complete tree)的数组形式存储; 树的应用,Huffman树的定义与应用; 三、查找(search)内容: 查找的基本概念;对线性关系结构的查找,顺序查找,二分查找; Hash查找法,常见的Hash函数(直接定址法,随机数法),hash冲突的概念, 解决冲突的方法(开散列方法/拉链法,闭散列方法/开址定址法),二次聚集现象; BST树定义,性质,ADT及其实现,BST树查找,插入,删除算法; 平衡树 (AVL) 的定义,性质,ADT及其实现,平衡树查找,插入算法,平衡因子的概念; 优先队列与堆,堆的定义,堆的生成,调整算法;范围查询; 四、排序内容: 排序基本概念;插入排序,希尔排序,选择排序,快速排序,合并排序,基数排序等排序算法基本思想,算法代码及基本的时间复杂度分析 五、图内容： 图的基本概念;图的存储结构,邻接矩阵,邻接表;图的遍历,广度度优先遍历和深度优先遍历;最小生成树基本概念,Prim算法,Kruskal算法;最短路径问题,广度优先遍历算法,Dijkstra算法,Floyd算法;拓扑排序 第二部分计算机系统基础考试题型：问答、分析、编程 总分：40分 一 、处理器体系结构内容：CPU中的时序电路、单周期处理器的设计、流水线处理器的基本原理、Data Hazard的处理、流水线设计中的其他问题 二、优化程序性能内容：优化程序性能、优化编译器的能力和局限性以及表示程序性能、特定体系结构或应用特性的性能优化、限制因素、确认和消除性能瓶颈 三、存储器结构及虚拟存储器内容：局部性、存储器层级结构、计算机高速缓存器原理、高速缓存对性能的影响、地址空间、虚拟存储器、虚拟内存的管理、翻译和映射、TLB、动态存储器分配和垃圾收集 四、链接、进程及并发编程内容：静态链接、目标文件、符号和符号表、重定位和加载、动态链接库、异常和进程、进程控制和信号、进程间的通信、进程间信号量的控制、信号量，各种并发编程模式，共享变量和线程同步，其他并行问题 五、系统级I/O和网络编程内容：I/O相关概念、文件及文件操作、共享文件、网络编程、客户端-服务器模型，套接字接口、HTTP请求，Web服务器 第三部分软件工程考试题型：概念问答题、实践案例题 总分：50分 一、软件过程软件过程的概念；经典软件过程模型的特点（瀑布模型、增量模型、演化模型、统一过程模型）；过程评估与CMM/CMMI的基本概念；敏捷宣言与敏捷过程的特点。 二、软件需求软件需求的概念；需求工程的基本过程；分层数据流模型；用例和场景建模及其UML表达（用例图、活动图、泳道图、顺序图）；数据模型建模及其UML表达（类图）；行为模型建模及其UML表达（状态机图）。 三、软件设计与构造软件体系结构及体系结构风格的概念；设计模式的概念；模块化设计的基本思想及概念（抽象、分解、模块化、封装、信息隐藏、功能独立）；软件重构的概念；软件体系结构的UML建模（包图、类图、构件图、顺序图、部署图）；接口的概念；面向对象设计原则（开闭原则、Liskov替换原则、依赖转置原则、接口隔离原则）；内聚与耦合的概念、常见的内聚和耦合类型。 四、软件测试软件测试及测试用例的概念；单元测试、集成测试、确认测试、系统测试、回归测试的概念；调试的概念、调试与测试的关系；测试覆盖度的概念；白盒测试、黑盒测试的概念；代码圈复杂度的计算方法；白盒测试中的基本路径测试方法；黑盒测试中的等价类划分方法。]]></content>
      <categories>
        <category>复习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[详解神经网络]]></title>
    <url>%2F2018%2F07%2F20%2F%E8%AF%A6%E8%A7%A3%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[详解神经网络神经网络的结构神经网络由三部分组成，分别是最左边的输入层，隐藏层（实际应用中远远不止一层）和最右边的输出层。层与层之间用线连接在一起，每条连接线都有一个对应的权重值 w，除了输入层，一般来说每个神经元还有对应的偏置 b。 除了输入层的神经元，每个神经元都会有加权求和得到的输入值 z 和将 z 通过 Sigmoid 函数（也即是激活函数）非线性转化后的输出值 a，他们之间的计算公式如下其中，公式里面的变量l和j表示的是第 l 层的第 j 个神经元，ij 则表示从第 i 个神经元到第 j 个神经元之间的连线，w 表示的是权重，b 表示的是偏置，后面这些符号的含义大体上与这里描述的相似，所以不会再说明。下面的 Gif 动图可以更加清楚每个神经元输入输出值的计算方式（注意，这里的动图并没有加上偏置，但使用中都会加上）使用激活函数的原因是因为线性模型（无法处理线性不可分的情况）的表达能力不够，所以这里通常需要加入 Sigmoid 函数来加入非线性因素得到神经元的输出值。 可以看到 Sigmoid 函数的值域为 (0,1) ，若对于多分类任务，输出层的每个神经元可以表示是该分类的概率。当然还存在其他的激活函数，他们的用途和优缺点也都各异。 BP 算法执行的流程（前向传递和逆向更新）在手工设定了神经网络的层数，每层的神经元的个数，学习率 η（下面会提到）后，BP 算法会先随机初始化每条连接线权重和偏置，然后对于训练集中的每个输入 x 和输出 y，BP 算法都会先执行前向传输得到预测值，然后根据真实值与预测值之间的误差执行逆向反馈更新神经网络中每条连接线的权重和每层的偏好。在没有到达停止条件的情况下重复上述过程。 其中，停止条件可以是下面这三条 权重的更新低于某个阈值的时候 预测的错误率低于某个阈值 达到预设一定的迭代次数 譬如说，手写数字识别中，一张手写数字1的图片储存了28*28 = 784个像素点，每个像素点储存着灰度值(值域为[0,255])，那么就意味着有784个神经元作为输入层，而输出层有10个神经元代表数字0~9，每个神经元取值为0~1，代表着这张图片是这个数字的概率。 每输入一张图片（也就是实例），神经网络会执行前向传输一层一层的计算到输出层神经元的值，根据哪个输出神经元的值最大来预测输入图片所代表的手写数字。 然后根据输出神经元的值，计算出预测值与真实值之间的误差，再逆向反馈更新神经网络中每条连接线的权重和每个神经元的偏好。 前向传输（Feed-Forward） 从输入层=&gt;隐藏层=&gt;输出层，一层一层的计算所有神经元输出值的过程。 逆向反馈（Back Propagation） 因为输出层的值与真实的值会存在误差，我们可以用均方误差来衡量预测值和真实值之间的误差。 均方误差 逆向反馈的目标就是让E函数的值尽可能的小，而每个神经元的输出值是由该点的连接线对应的权重值和该层对应的偏好所决定的，因此，要让误差函数达到最小，我们就要调整w和b值， 使得误差函数的值最小。 权重和偏置的更新公式对目标函数 E 求 w 和 b 的偏导可以得到 w 和 b 的更新量，下面拿求 w 偏导来做推导。其中 η 为学习率，取值通常为 0.1 ~ 0.3,可以理解为每次梯度所迈的步伐。注意到 w_hj 的值先影响到第 j 个输出层神经元的输入值a，再影响到输出值y，根据链式求导法则有： 使用链式法则展开对权重求偏导 根据神经元输出值 a 的定义有：对函数 z 求 w 的偏导 Sigmoid 求导数的式子如下，从式子中可以发现其在计算机中实现也是非常的方便： 所以 则权重 w 的更新量为： 类似可得 b 的更新量为：但这两个公式只能够更新输出层与前一层连接线的权重和输出层的偏置，原因是因为 δ 值依赖了真实值y这个变量，但是我们只知道输出层的真实值而不知道每层隐藏层的真实值，导致无法计算每层隐藏层的 δ 值，所以我们希望能够利用 l+1 层的 δ 值来计算 l 层的 δ 值，而恰恰通过一些列数学转换后可以做到，这也就是逆向反馈名字的由来，公式如下:从式子中我们可以看到，我们只需要知道下一层的权重和神经元输出层的值就可以计算出上一层的 δ 值，我们只要通过不断的利用上面这个式子就可以更新隐藏层的全部权重和偏置了。 在推导之前请先观察下面这张图：首先我们看到 l 层的第 i 个神经元与 l+1 层的所有神经元都有连接，那么我们可以将 δ 展开成如下的式子：也即是说我们可以将 E 看做是 l+1 层所有神经元输入值的 z 函数，而上面式子的 n 表示的是 l+1 层神经元的数量，再进行化简后就可以得到上面所说的式子。 卷积神经网络卷积神经网络采用了三种基本概念：局部感受野（local receptive fields），共享权重（shared weights），和混合（pooling）。让我们逐个看下： 局部感受野在之前看到的全连接层的网络中，输入被描绘成纵向排列的神经元。但在一个卷积网络中，把输入看作是一个2828的方形排列的神经元更有帮助，其值对应于我们用作输入的28 28 的像素光强度：和通常一样，我们把输入像素连接到一个隐藏神经元层。但是我们不会把每个输入像素连接到每个隐藏神经元。相反，我们只是把输入图像进行小的，局部区域的连接。 说的确切一点，第一个隐藏层中的每个神经元会连接到一个输入神经元的一个小区域，例如，一个5*5的区域，对应于25个输入像素。所以对于一个特定的隐藏神经元，我们可能有看起来像这样的连接：这个输入图像的区域被称为隐藏神经元的局部感受野。它是输入像素上的一个小窗口。 每个连接学习一个权重。而隐藏神经元同时也学习一个总的偏置。你可以把这个特定的隐藏神经元看作是在学习分析它的局部感受野。 我们然后在整个输入图像上交叉移动局部感受野。对于每个局部感受野，在第一个隐藏层中有一个不同的隐藏神经元。为了正确说明，让我们从左上角开始一个局部感受野： 然后我们往右一个像素（即一个神经元）移动局部感受野，连接到第二个隐藏神经元：如此重复，构建起第一个隐藏层。注意如果我们有一个2828的输入图像，5 5的局部感受野，那么隐藏层中就会有24 * 24个神经元。这是因为在抵达右边（或者底部）的输入图像之前，我们只能把局部感受野横向移动23个神经元（或者往下23个神经元）。 我显示的局部感受野每次移动一个像素。实际上，有时候会使用不同的跨距。例如，我可以往右（或下）移动2个像素的局部感受野，这种情况下我们使用了1个跨距。在这章里我们大部分时候会固定使用1的跨距，但是值得知道人们有时用不同的跨距试验2。 共享权重和偏置：我已经说过每个隐藏神经元具有一个偏置和连接到它的局部感受野的55权重。我没有提及的是我们打算24 24隐藏神经元中的每一个使用相同的权重和偏置。换句话说，对第j,k个隐藏神经元，输出为： 这意味着第一个隐藏层的所有神经元检测完全相同的特征，只是在输入图像的不同位置。要明白为什么是这个道理，把权重和偏置设想成隐藏神经元可以挑选的东西，例如，在一个特定的局部感受野的垂直边缘。这种能力在图像的其它位置也很可能是有用的。因此，在图像中应用相同的特征检测器是非常有用的。用稍微更抽象的术语，卷积网络能很好地适应图像的平移不变性：例如稍稍移动一幅猫的图像，它仍然是一幅猫的图像。 因为这个原因，我们有时候把从输入层到隐藏层的映射称为一个特征映射。我们把定义特征映射的权重称为共享权重。我们把以这种方式定义特征映射的偏置称为共享偏置。共享权重和偏置经常被称为一个卷积核或者滤波器。在文献中，人们有时以稍微不同的方式使用这些术语，对此我不打算去严格区分；稍后我们会看一些具体的例子。 目前我描述的网络结构只能检测一种局部特征的类型。为了完成图像识别我们需要超过一个的特征映射。所以一个完整的卷积层由几个不同的特征映射组成： 共享权重和偏置的一个很大的优点是，它大大减少了参与的卷积网络的参数 池化/采样层 除了刚刚描述的卷积层，卷积神经网络也包含池化层（pooling layers）。池化层通常紧接着在卷积层之后使用。它要做的是简化从卷积层输出的信息。 详细地说，一个池化层取得从卷积层输出的每一个特征映射6并且从它们准备一个凝缩的特征映射。例如，混合层的每个单元可能概括了前一层的一（比如）22 的区域。作为一个具体的例子，一个常见的混合的程序被称最大值混合（max-pooling）。在最大池化层中，一个pooling单元简单地输出其2 2输入区域的最大激活值，正如下图说明的： 注意既然从卷积层有24 24个神经元输出，池化后我们得到12 12个神经元。 正如上面提到的，卷积层通常包含超过一个特征映射。我们将最大值混合分别应用于每一个特征映射。所以如果有三个特征映射，组合在一起的卷积层和池化层层看起来像这样： 综合在一起： 我们现在可以把这些思想都放在一起来构建一个完整的卷积神经网络。它和我们刚看到的架构相似，但是有额外的一层 10个输出神经元，对应于个可能的 MNIST 数字（’0’，’1’，’2’等）：]]></content>
  </entry>
  <entry>
    <title><![CDATA[反向传播]]></title>
    <url>%2F2018%2F03%2F20%2F%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD(backpropagation)%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84%2F</url>
    <content type="text"><![CDATA[反向传播(backpropagation)是如何工作的反向传播算法最早于上世纪70年代被提出，但是直到1986年，由David Rumelhart, Geoffrey Hinton, 和Ronald Williams联合发表了一篇著名论文之后，人们才完全认识到这个算法的重要性。这篇论文介绍了几种神经网络，在这些网络的学习中，反向传播算法比之前提出的方法都要快。这使得以前用神经网络不可解的一些问题，现在可以通过神经网络来解决。今天，反向传播算法是神经网络学习过程中的关键（workhorse）所在。反向传播算法的核心是一个偏微分表达式，表示代价函数对网络中的权重（或者偏置）求偏导。这个式子告诉我们，当我们改变权重和偏置的时候，代价函数的值变化地有多快。尽管这个式子有点复杂，这个式子也是很漂亮的，它的每一个部分都有自然的，直觉上的解释。因此，反向传播不仅仅是一种快速的学习算法，它能够让我们详细深入地了解改变权重和偏置的值是如何改变整个网络的行为的。这是非常值得深入学习的。 关于代价函数的两个架设反向传播算法的目标是计算代价函数对神经网络中出现的所有权重和偏置的偏导数和 第一条假设是代价函数能够被写成的形式，其中是每个独立训练样本的代价函数。在代价函数为平方代价函数的情况下，一个训练样本的代价是。该假设对于本书中涉及到的其它所有代价函数都成立。我们需要上述假设的原因是，反向传播实际上是对单个训练数据计算偏导数和。然后通过对所有训练样本求平均值获得和。事实上，有了这个假设，我们可以认为训练样本是固定的，然后把代价去掉下标表示为。最终我们会重新把加回公式，但目前为了简便我们将它隐去。 第二条假设是它可以写成关于神经网络输出结果的函数：平方代价函数满足该要求，因为单一训练样本的二次代价可以表示为：这是一个关于输出激活值的函数。显然，该代价函数也依赖于期望的输出，所以你可能疑惑为什么我们不把代价视为关于的函数。记住，输入的训练样本是固定的，因此期望的输出也是固定的。需要注意，我们不能通过改变权值或偏置来修改它，换句话说，它不是神经网络所学习的东西。所以把视为只关于输出的函数是有道理的。在该函数中只是帮助定义函数的参数。 Hadamard积反向传播算法是以常见线性代数操作为基础——诸如向量加法，向量与矩阵乘法等运算。但其中一个操作相对不是那么常用。具体来讲，假设和是两个有相同维数的向量。那么我们用来表示两个向量的对应元素(elementwise)相乘。因此的元素。例如，这种对应元素相乘有时被称为Hadamard积（Hadamard product）或Schur积(Schur product)。我们将称它为Hadamard积。优秀的矩阵库通常会提供Hadamard积的快速实现，这在实现反向传播时将会有用。 反向传播背后的四个基本等式反向传播(backpropagation)能够帮助解释网络的权重和偏置的改变是如何改变代价函数的。归根结底，它的意思是指计算偏导数和。但是为了计算这些偏导数，我们首先介绍一个中间量，，我们管它叫做层的神经元的错误量(error)。反向传播会提供给我们一个用于计算错误量的流程，能够把和、关联起来。为了理解错误量是如何定义的，想象一下在我们的神经网络中有一个恶魔： 这个恶魔位于层的神经元。当神经元的输入进入时，这个恶魔扰乱神经元的操作。它给神经元的加权输入添加了一点改变，这就导致了神经元的输出变成了，而不是之前的。这个改变在后续的网络层中传播，最终使全部代价改变了。而今，这个恶魔变成了一个善良的恶魔，它试图帮助你改善代价，比如，它试图找到一个能够让代价变小。假设是一个很大的值（或者为正或者为负）。然后这个善良的恶魔可以通过选择一个和符号相反的使得代价降低。相比之下，如果接近于0，那么这个恶魔几乎不能通过扰乱加权输入改善多少代价。在一定范围内这个善良的恶魔就可以分辨出，这个神经元已经接近于最佳状态1。至此，有了一种启发式的感觉：可以用来衡量神经元里的错误量。 1.输出层中关于错误量的等式（计算最后一层神经网络产生的错误）：推导过程 这是一种非常自然的表达。右侧的第一项，就是用于测量输出激活代价改变有多快的函数。举个例子，如果并不太依赖于某个特别的输出神经元，那么就会很小，这是我们所期望的。右侧的第二项，用于测量处的激活函数改变有多快。你应该注意到(BP1)中的每一项都是容易计算的。特别的，当计算神经网络的行为时就计算了，而计算也仅仅是一小部分额外的开销。当然了，的确切形式依赖于代价函数的形式。然而，如果提供了代价函数，大家也应该知道计算也不会有什么困难。举个例子，如果我们使用平方代价函数，即，那么，这显然很容易计算。等式(BP1)是的分量形式。它是一个完美的表达式，但并不是我们想要的基于矩阵的形式，那种矩阵形式可以很好的用于反向传播。然而，我们可以很容易把等式重写成基于矩阵的形式，就像：其中，是一个向量，它是由组成的。你可以把看做现对于输出激活的的改变速率。很容易看出来等式(BP1)和(BP1a)是等价的，基于这个原因我们从现在开始将使用(BP1)交替地指代两个等式。举个例子，在使用平方代价函数的情况下我们有，所以完整的基于矩阵的(BP1)的形式变为：就像你所看到的，表达式里的每一项都拥有一个漂亮的向量形式，并且很容易使用一个库来计算，比如Numpy。 2.依据下一层错误量获取错误量的等式（由后往前，计算每一层神经网络产生的错误）： 其中，是层的权重矩阵的转置。这个等式看着有些复杂，但是每一项都有很好的解释。假设我们知道层的错误量。当我们使用转置权值矩阵的时候，我们可以凭借直觉认为将错误反向（backward）移动穿过网络，带给我们某种测量层输出的错误量方法。然后我们使用Hadamard乘积。这就是将错误量反向移动穿过层的激活函数，产生了层的加权输入的错误量。通过结合(BP2)和(BP1)我们可以计算网络中任意一层的错误量。我们开始使用(BP1)来计算，然后应用等式(BP2)来计算，然后再次应用等式(BP2)来计算，以此类推，反向通过网络中的所有路径。 3.网络的代价函数相对于偏置的改变速率的等式（计算偏置的梯度）： 也就是说，错误量完全等于改变速率。这是一个很好的消息，因为(BP1)和(BP2)已经告诉我们如何计算。我们把(BP3)重写成如下的简略形式：这可以理解成可以和偏置在相同的神经元中被估计。 4.网络的代价函数相对于权重的改变速率的等式（计算权重的梯度）：： 这个等式告诉我们如何依据和来计算偏导，而这两个量我们已经知道如何计算了。这个等式可以重写成如下含有少量下标的形式：可以这么理解，是神经元的激活量，输入到权重中，是神经元的错误量，从权重输出。观察这个权重，两个神经元通过这个权重连接起来，我们可以这样描画出来： 等式(32)的一个很好的结论是当激活量很小的时候，，梯度项也将会趋近于很小。在这种情况下，我们说权重学习得很慢，也就是说在梯度下降的时候并没有改变很多。换而言之，等式(BP4)的一个结果就是从低激活量神经元里输出的权重会学习缓慢。 反向传播算法反向传播等式为我们提供了一个计算代价函数梯度的方法。下面让我们明确地写出该算法： 输入 :计算输入层相应的激活函数值。 正向传播：对每个，计算和。 输出误差 ：计算向量。 将误差反向传播：对每个计算 输出：代价函数的梯度为和通过以上算法就能看出它为什么叫反向传播算法。我们从最后一层开始，反向计算错误向量。在神经网络中反向计算误差可能看起来比较奇怪。但如果回忆反向传播的证明过程，会发现反向传播的过程起因于代价函数是关于神经网络输出值的函数。为了了解代价函数是如何随着前面的权重和偏置改变的，我们必须不断重复应用链式法则，通过反向的计算得到有用的表达式。 为什么说反向传播算法很高效？为什么说反向传播算法很高效？要回答这个问题，让我们来考虑另一种计算梯度的方式。设想现在是神经网络研究的早期阶段，大概是在上世纪50年代或60年代左右，并且你是第一个想到使用梯度下降方法来进行训练的人！但是要实现这个想法，你需要一种计算代价函数梯度的方式。你回想了你目前关于演算的知识，决定试一下是否能用链式法则来计算梯度。但是琢磨了一会之后发现，代数计算看起来非常复杂，你也因此有些失落。所以你尝试着寻找另一种方法。你决定把代价单独当做权重的函数（我们一会再来讨论偏置）。将权重写作，并且要对某个权重计算。一个很明显的计算方式是使用近似：其中是一个大于零的小正数。换句话说，我们可以通过计算两个差距很小的wj的代价，然后利用等式(46)来估计。我们可以利用相同的思想来对偏置求偏导。这种方式看起来很不错。它的概念很简单，实现起来也很简单，只需要几行代码就可以。当然了，他看起来要比使用链式法则来计算梯度靠谱多了！然而遗憾的是，虽然这种方式看起来很美好，但当用代码实现之后就会发现，它实在是太慢了。要理解其中的原因的话，设想在我们的神经网络中有一百万个权重，对于每一个不同的权重，为了计算，我们需要计算。这意味着为了计算梯度，我们需要计算一百万次代价函数，进而对于每一个训练样例，都需要在神经网络中前向传播一百万次。我们同样需要计算，因此总计需要一百万零一次前向传播。反向传播的优点在于它仅利用一次前向传播就可以同时计算出所有的偏导，随后也仅需要一次反向传播。大致来说，反向传播算法所需要的总计算量与两次前向传播的计算量基本相等（这应当是合理的，但若要下定论的话则需要更加细致的分析。合理的原因在于前向传播时主要的计算量在于权重矩阵的乘法计算，而反向传播时主要的计算量在于权重矩阵转置的乘法。很明显，它们的计算量差不多）。这与基于等式(46)的方法所需要的一百万零一次前向传播相比，虽然反向传播看起来更复杂一些，但它确实更更更更更快。这种加速方式在1986年首次被人们所重视，极大地拓展了神经网络能够适用的范围，也导致了神经网络被大量的应用。当然了，反向传播算法也不是万能的。在80年代后期，人们终于触及到了性能瓶颈，在利用反向传播算法来训练深度神经网络（即具有很多隐含层的网络）时尤为明显。在本书后面的章节中我们将会看到现代计算机以及一些非常聪明的新想法是如何让反向传播能够用来训练深度神经网络的。 反向传播：整体描述正如我之前所阐述的，反向传播涉及了两个谜题。第一个谜题是，这个算法究竟在做什么？我们之前的描述是将错误量从输出层反向传播。但是，我们是否能够更加深入，对这些矩阵、向量的乘法背后作出更加符合直觉的解释？第二个谜题是，人们一开始是如何发现反向传播算法的？按照算法流程一步步走下来，或者证明算法的正确性，这是一回事。但这并不代表你能够理解问题的本质从而能够从头发现这个算法。是否有一条合理的思维路线使你能够发现反向传播算法？在本节中，我会对这两个谜题作出解释。为了更好地构建的反向传播算法在做什么的直觉，让我们假设我们对网络中的某个权重做出了一个小的改变量： 这个改变量会导致与其相关的神经元的输出激活值的改变： 以此类推，会引起下一层的所有激活值的改变： 这些改变会继续引起再下一层的改变、下下层…依次类推，直到最后一层，然后引起代价函数的改变： 代价函数的改变量与最初权重的改变量是有关的，关系是下面这个等式这表明了计算的一种可能方法是，计算上的一个小改变量经过正向传播，对引起了多大的改变量。如果我们能够通过小心翼翼的计算做到这点，那我们就可以计算出。让我们尝试来写一下计算过程。改变量对层的第个神经元的激活值带来了的改变量。它的大小是激活值改变量会引起下一层（第层）的所有激活值都改变。我们先关注其中的一个结点，， 它产生的改变量是：将等式(48)带入其中，得到：当然，改变量会继续造成下一层的激活值的改变。实际上，我们可以想象一条从到的路径，其中每一个结点的激活值的改变都会引起下一层的激活值的改变，最终引起输出层的代价的改变。如果这条路径是，那么最终的改变量是这样，我们使用了一系列的形式的项，对应了路径上的每一个结点，包括最终项。这就计算出了在神经网络的这条路径上，最初的改变量引起了多大的改变。当然，由的改变量影响代价改变的路径选择是很多的，我们现在只考虑了其中的一条路径。为了计算最终总共的改变量，很显然我们应该对所有可能的路径对其带来的改变量进行求和：其中，我们对每条路径中所有出现的神经元都进行求和。与等式(47)进行比较，我们得到：等式(53)看上去很复杂。不过，它在直觉上很容易理解。我们计算出了相对于网络中的一个权重的变化速率。这个等式告诉我们的是，每一条连接两个神经元的边都可以对应一个变化速率，这个变化速率的大小是后一个神经元对前一个神经元的偏导。连接第一个权重和第一个神经元的边对应的变化速率是。一条路径对应的变化速率恰好是路径上的变化速率的连乘。总变化速率是从起始权重到最终代价上的所有可能的路径的变化速率的总和。用图片来说明这个过程，对于一条路径来说： 目前为止我所阐述的是一种启发式的观点，当你困惑于神经网络中的权重时，可以通过这种观点来思考。下面我会给你一些简要的思路使你可以更进一步的完善这个观点。首先，你可以显式地计算出等式53中的每一项的偏导表达式。这很容易做到，只需要一点计算量就行。做完之后，你可以尝试将所有的求和通过矩阵的形式来表示。这个过程会有一点无聊，并且需要一些毅力，但是不会特别的困难。随后，你可以尝试尽可能的将表达式简化，最终你会发现你得到了反向传播算法！所以，你可以将反向传播算法看作是一种对所有路径上的所有变化率进行求和的方法。或者用另外一种方式来说，反向传播算法是一种很聪明的方法，当小扰动沿着网络传播、到达输出并影响代价的过程中，它能够记录其对相应权重（和偏置）的影响量。我的解释到此为止了。这可能有点复杂，并且需要仔细思考所有的细节。如果你乐于接受挑战，你可能会很享受这个过程。如果不是，我希望我的这些想法能够给你一些关于反向传播在做什么的启发。那关于其它的谜题呢——反向传播最初是如何被发现的？实际上，如果你一路看下来我的阐述，你能够找到关于反向传播算法的一种证明。不幸的是，完整的证明实际上比我在本章的描述更长更复杂。那这个相对更简单（但更玄虚）的证明是如何发现的呢？当你把试图把完整的证明中的所有细节写出来时，你会发现有一些很显然能够被简化的形式。你做完这些简化，会得到一个简短一些的证明。然后你又会发现一些可以简化的内容。当你重复几次这个过程之后，你会得到本章中的这个简短的证明，但是它有点晦涩，因为其中所有复杂的结构都被简化掉了！我希望你能够相信我，完整的证明与本章中简短的证明没有什么本质区别。我只是对证明过程做了很多简化的工作。 转自gitbook Neural Networks and Deep Learning 翻译https://www.gitbook.com/book/hit-scir/neural-networks-and-deep-learning-zh_cn/details]]></content>
  </entry>
</search>
